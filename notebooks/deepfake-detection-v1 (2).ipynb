{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5380830,"sourceType":"datasetVersion","datasetId":3120670},{"sourceId":10125851,"sourceType":"datasetVersion","datasetId":6248577}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install opencv-python-headless facenet-pytorch ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:47:03.913493Z","iopub.execute_input":"2025-10-17T17:47:03.914314Z","iopub.status.idle":"2025-10-17T17:47:07.940966Z","shell.execute_reply.started":"2025-10-17T17:47:03.914289Z","shell.execute_reply":"2025-10-17T17:47:07.940076Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os, random, math, cv2, glob, shutil, json\nimport numpy as np\nimport torch\nfrom facenet_pytorch import MTCNN\nfrom pathlib import Path\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nCELEB_ROOT = \"/kaggle/input/celeb-df-v2\"\nFFPP_ROOT  = \"/kaggle/input/ff-c23/FaceForensics++_C23\"\n\nOUT_ROOT = \"/kaggle/working/data\"\nFFPP_OUT = f\"{OUT_ROOT}/FFpp_c23_train\"  \nCELEB_OUT= f\"{OUT_ROOT}/CelebDF_test\"     \n\nMAX_VIDEOS_PER_CLASS = 1000     \nFRAMES_PER_VIDEO     = 1        \nIMAGE_SIZE           = 224      \nMTCNN_MARGIN         = 20       \n\nfor p in [FFPP_OUT, CELEB_OUT]:\n    Path(p).mkdir(parents=True, exist_ok=True)\n    Path(f\"{p}/real\").mkdir(parents=True, exist_ok=True)\n    Path(f\"{p}/fake\").mkdir(parents=True, exist_ok=True)\n\nmtcnn = MTCNN(\n    image_size=IMAGE_SIZE,\n    margin=MTCNN_MARGIN,\n    select_largest=True,\n    post_process=True,\n    device=device,\n    keep_all=False\n)\nprint(\"MTCNN ready.\")\n\ndef list_videos_celebdf(root: str) -> dict:\n    real_dirs = [f\"{root}/Celeb-real\", f\"{root}/YouTube-real\"]\n    fake_dirs = [f\"{root}/Celeb-synthesis\"]\n    real = []\n    for d in real_dirs:\n        if os.path.isdir(d):\n            real += sorted([str(p) for p in Path(d).glob(\"*.mp4\")])\n    fake = []\n    for d in fake_dirs:\n        if os.path.isdir(d):\n            fake += sorted([str(p) for p in Path(d).glob(\"*.mp4\")])\n    return {\"real\": real, \"fake\": fake}\n\n\ndef list_videos_ffpp(root: str) -> dict:\n    real_dir = f\"{root}/original\"\n    fake_dirs = [\n        f\"{root}/DeepFakeDetection\",\n        f\"{root}/Deepfakes\",\n        f\"{root}/Face2Face\",\n        f\"{root}/FaceShifter\",\n        f\"{root}/FaceSwap\",\n        f\"{root}/NeuralTextures\",\n    ]\n    real = sorted([str(p) for p in Path(real_dir).glob(\"*.mp4\")]) if os.path.isdir(real_dir) else []\n    fake = []\n    for d in fake_dirs:\n        if os.path.isdir(d):\n            fake += sorted([str(p) for p in Path(d).glob(\"*.mp4\")])\n    return {\"real\": real, \"fake\": fake}\n\n\ndef sample_subset(paths: list, k: int) -> list:\n    if len(paths) <= k:\n        return paths\n    rng = random.Random(SEED)\n    return rng.sample(paths, k)\n\n\ndef evenly_spaced_indices(n_frames: int, count: int) -> list:\n    if n_frames <= 0 or count <= 0:\n        return []\n    count = min(count, n_frames)\n    return sorted(set([int(round(x)) for x in np.linspace(0, n_frames - 1, num=count)]))\n\n\ndef save_face_crop_from_frame(bgr_frame: np.ndarray, save_path: str) -> dict | None:\n    rgb = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)\n    face_tensor = mtcnn(rgb, save_path=save_path)\n    if face_tensor is None:\n        return None\n    boxes, probs = mtcnn.detect(rgb)\n    if boxes is None or len(boxes) == 0:\n        return None\n    areas = [(x2-x1)*(y2-y1) for (x1,y1,x2,y2) in boxes]\n    idx = int(np.argmax(areas))\n    (x1, y1, x2, y2) = [float(v) for v in boxes[idx]]\n    score = float(probs[idx]) if probs is not None else None\n    return {\"bbox\": [x1, y1, x2, y2], \"score\": score}\n\n\ndef extract_faces_from_video(video_path: str, out_dir: str, label: str, frames_per_video: int) -> int:\n    os.makedirs(f\"{out_dir}/{label}\", exist_ok=True)\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        return 0\n\n    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    indices = evenly_spaced_indices(frame_count, frames_per_video)\n\n    saved = 0\n    video_stem = Path(video_path).stem\n    for idx in indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ok, frame = cap.read()\n        if not ok or frame is None:\n            continue\n        save_path = f\"{out_dir}/{label}/{video_stem}_f{idx:06d}.png\"\n        info = save_face_crop_from_frame(frame, save_path)\n        if info is not None:\n            saved += 1\n    cap.release()\n    return saved\n\n\ndef process_dataset(video_dict: dict, out_root: str, frames_per_video: int, max_per_class: int) -> dict: \n    summary = {\"real\": {\"videos\": 0, \"crops\": 0}, \"fake\": {\"videos\": 0, \"crops\": 0}}\n    for label in [\"real\", \"fake\"]:\n        vids = sample_subset(video_dict.get(label, []), max_per_class)\n        print(f\"Found {len(video_dict.get(label, []))} {label} videos; using {len(vids)}.\")\n        for v in vids:\n            crops = extract_faces_from_video(v, out_root, label, frames_per_video)\n            summary[label][\"videos\"] += 1\n            summary[label][\"crops\"]  += crops\n    return summary\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:47:07.942671Z","iopub.execute_input":"2025-10-17T17:47:07.942926Z","iopub.status.idle":"2025-10-17T17:47:07.984102Z","shell.execute_reply.started":"2025-10-17T17:47:07.942904Z","shell.execute_reply":"2025-10-17T17:47:07.983502Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nMTCNN ready.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"ffpp_vids = list_videos_ffpp(FFPP_ROOT)\nffpp_summary = process_dataset(\n    video_dict=ffpp_vids,\n    out_root=FFPP_OUT,\n    frames_per_video=FRAMES_PER_VIDEO,\n    max_per_class=MAX_VIDEOS_PER_CLASS\n)\nprint(\"FF++ summary:\", ffpp_summary)\n\nceleb_vids = list_videos_celebdf(CELEB_ROOT)\nceleb_summary = process_dataset(\n    video_dict=celeb_vids,\n    out_root=CELEB_OUT,\n    frames_per_video=FRAMES_PER_VIDEO,\n    max_per_class=MAX_VIDEOS_PER_CLASS\n)\nprint(\"Celeb-DF summary:\", celeb_summary)\n\nprint(\"\\nOutput folders:\")\nfor p in [f\"{FFPP_OUT}/real\", f\"{FFPP_OUT}/fake\", f\"{CELEB_OUT}/real\", f\"{CELEB_OUT}/fake\"]:\n    n = len(list(Path(p).glob(\"*.png\")))\n    print(f\"{p}: {n} images\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:47:07.984871Z","iopub.execute_input":"2025-10-17T17:47:07.985131Z","iopub.status.idle":"2025-10-17T17:56:11.352908Z","shell.execute_reply.started":"2025-10-17T17:47:07.985106Z","shell.execute_reply":"2025-10-17T17:56:11.352242Z"}},"outputs":[{"name":"stdout","text":"Found 1000 real videos; using 1000.\nFound 6000 fake videos; using 1000.\nFF++ summary: {'real': {'videos': 1000, 'crops': 1000}, 'fake': {'videos': 1000, 'crops': 993}}\nFound 890 real videos; using 890.\nFound 5639 fake videos; using 1000.\nCeleb-DF summary: {'real': {'videos': 890, 'crops': 889}, 'fake': {'videos': 1000, 'crops': 1000}}\n\nOutput folders:\n/kaggle/working/data/FFpp_c23_train/real: 1000 images\n/kaggle/working/data/FFpp_c23_train/fake: 771 images\n/kaggle/working/data/CelebDF_test/real: 889 images\n/kaggle/working/data/CelebDF_test/fake: 1000 images\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip -q install timm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:56:11.354769Z","iopub.execute_input":"2025-10-17T17:56:11.355180Z","iopub.status.idle":"2025-10-17T17:56:13.336372Z","shell.execute_reply.started":"2025-10-17T17:56:11.355158Z","shell.execute_reply":"2025-10-17T17:56:13.335329Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import os, math\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\n\nIMAGE_SIZE = 224\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ndata_tfms = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225]),\n])\n\ntrain_dir = FFPP_OUT            \ntest_dir  = CELEB_OUT            \n\ntrain_ds = ImageFolder(train_dir, transform=data_tfms)\ntest_ds  = ImageFolder(test_dir,  transform=data_tfms)\n\nBATCH_SIZE = 32\nNUM_WORKERS = 4\nPIN_MEMORY = True if device == \"cuda\" else False\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=False)\ntest_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=False)\n\ndef _count_per_class(ds):\n    counts = [0, 0]\n    for _, y in ds.samples:\n        counts[y] += 1\n    return counts\n\ntr_counts = _count_per_class(train_ds)\nte_counts = _count_per_class(test_ds)\n\nprint(f\"Train counts  → real: {tr_counts[0]} | fake: {tr_counts[1]} | total: {len(train_ds)}\")\nprint(f\"Test  counts  → real: {te_counts[0]} | fake: {te_counts[1]} | total: {len(test_ds)}\")\nprint(\"Classes mapping:\", train_ds.class_to_idx)  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:56:13.337360Z","iopub.execute_input":"2025-10-17T17:56:13.337647Z","iopub.status.idle":"2025-10-17T17:56:13.357725Z","shell.execute_reply.started":"2025-10-17T17:56:13.337624Z","shell.execute_reply":"2025-10-17T17:56:13.356990Z"}},"outputs":[{"name":"stdout","text":"Train counts  → real: 771 | fake: 1000 | total: 1771\nTest  counts  → real: 1000 | fake: 889 | total: 1889\nClasses mapping: {'fake': 0, 'real': 1}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EfficientViT(nn.Module):\n    def __init__(\n        self,\n        num_classes: int = 2,\n        backbone_name: str = \"efficientnet_b0\",\n        transformer_layers: int = 4,\n        transformer_heads: int = 4,\n        dropout: float = 0.1,\n        grid_size: int = 7,    \n    ):\n        super().__init__()\n        self.backbone = timm.create_model(backbone_name, pretrained=True, num_classes=0, global_pool=\"\")\n        self.feature_dim = getattr(self.backbone, \"num_features\", 1280)\n        self.grid_size = grid_size\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.feature_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + grid_size * grid_size, self.feature_dim))\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=self.feature_dim,\n            nhead=transformer_heads,\n            dim_feedforward=self.feature_dim * 4,\n            dropout=dropout,\n            activation=\"gelu\",\n            batch_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=transformer_layers)\n        self.pre_ln = nn.LayerNorm(self.feature_dim)\n\n        self.head = nn.Sequential(\n            nn.LayerNorm(self.feature_dim),\n            nn.Linear(self.feature_dim, 256),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(256, num_classes),\n        )\n\n    @torch.no_grad()\n    def _interpolate_pos_enc(self, tokens: torch.Tensor, H: int, W: int) -> torch.Tensor:\n        B, N, C = tokens.shape  # N = 1 + H*W\n        if H * W == self.grid_size * self.grid_size:\n            return self.pos_embed[:, : N, :]\n        cls_pe, grid_pe = self.pos_embed[:, :1, :], self.pos_embed[:, 1:, :]\n        grid_pe = grid_pe.view(1, self.grid_size, self.grid_size, C).permute(0, 3, 1, 2)\n        grid_pe = F.interpolate(grid_pe, size=(H, W), mode=\"bicubic\", align_corners=False)\n        grid_pe = grid_pe.permute(0, 2, 3, 1).reshape(1, H * W, C)\n        return torch.cat([cls_pe, grid_pe], dim=1)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        feat_map = self.backbone.forward_features(x)\n        B, C, H, W = feat_map.shape\n\n        tokens = feat_map.flatten(2).transpose(1, 2)         \n        cls = self.cls_token.expand(B, -1, -1)              \n        tokens = torch.cat([cls, tokens], dim=1)              \n\n        pe = self._interpolate_pos_enc(tokens, H, W)\n        tokens = tokens + pe\n\n        tokens = self.pre_ln(tokens)\n        tokens = self.transformer(tokens)                     \n\n        cls_out = tokens[:, 0, :]                             \n        logits = self.head(cls_out)                            \n        return logits\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nmodel = EfficientViT()\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\nmodel = model.to(device)\n\nxb, yb = next(iter(train_loader))\nxb = xb.to(device)\nwith torch.no_grad():\n    out = model(xb)\nprint(\"Batch:\", xb.shape, \"→ logits:\", out.shape)\n\ndef count_params(m):\n    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n\nprint(\"Trainable parameters:\", f\"{count_params(model):,}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:56:16.925212Z","iopub.execute_input":"2025-10-17T17:56:16.925493Z","iopub.status.idle":"2025-10-17T17:56:20.375936Z","shell.execute_reply.started":"2025-10-17T17:56:16.925468Z","shell.execute_reply":"2025-10-17T17:56:20.374911Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c347bb2609f44eeda9dfd6b9f37f0e69"}},"metadata":{}},{"name":"stdout","text":"Batch: torch.Size([32, 3, 224, 224]) → logits: torch.Size([32, 2])\nTrainable parameters: 83,116,158\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import math, time\nimport torch\nimport torch.nn as nn\nfrom torch.cuda.amp import GradScaler, autocast\n\nBASELINE_EPOCHS = 8      \nLR                = 3e-4\nWEIGHT_DECAY      = 1e-4\nLABEL_SMOOTHING   = 0.05\nMAX_NORM          = 1.0  \nUSE_AMP           = torch.cuda.is_available()\n\ntorch.backends.cudnn.benchmark = True\n\ncls_counts = [0, 0]\nfor _, y in train_ds.samples:\n    cls_counts[y] += 1\nN = float(len(train_ds))\ncls_weights = [N/(2.0*max(1, c)) for c in cls_counts]  \ncls_weights_t = torch.tensor(cls_weights, device=device, dtype=torch.float32)\n\ncriterion = nn.CrossEntropyLoss(weight=cls_weights_t, label_smoothing=LABEL_SMOOTHING)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=BASELINE_EPOCHS)\nscaler = GradScaler(enabled=USE_AMP)\n\ndef accuracy_from_logits(logits, targets):\n    preds = logits.argmax(dim=1)\n    return (preds == targets).float().mean().item()\n\ndef train_one_epoch(model, loader, optimizer, scaler):\n    model.train()\n    epoch_loss, epoch_acc, n = 0.0, 0.0, 0\n    for x, y in loader:\n        x = x.to(device, non_blocking=True)\n        y = y.to(device, non_blocking=True)\n\n        optimizer.zero_grad(set_to_none=True)\n        with autocast(enabled=USE_AMP):\n            logits = model(x)\n            loss = criterion(logits, y)\n        scaler.scale(loss).backward()\n        nn.utils.clip_grad_norm_(model.parameters(), MAX_NORM)\n        scaler.step(optimizer)\n        scaler.update()\n\n        bsz = x.size(0)\n        epoch_loss += loss.item() * bsz\n        epoch_acc  += accuracy_from_logits(logits.detach(), y) * bsz\n        n += bsz\n    return epoch_loss / n, epoch_acc / n\n\n@torch.no_grad()\ndef evaluate(model, loader):\n    model.eval()\n    epoch_loss, epoch_acc, n = 0.0, 0.0, 0\n    for x, y in loader:\n        x = x.to(device, non_blocking=True)\n        y = y.to(device, non_blocking=True)\n        with autocast(enabled=USE_AMP):\n            logits = model(x)\n            loss = criterion(logits, y)\n        bsz = x.size(0)\n        epoch_loss += loss.item() * bsz\n        epoch_acc  += accuracy_from_logits(logits, y) * bsz\n        n += bsz\n    return epoch_loss / n, epoch_acc / n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:56:20.376999Z","iopub.execute_input":"2025-10-17T17:56:20.377249Z","iopub.status.idle":"2025-10-17T17:56:20.391333Z","shell.execute_reply.started":"2025-10-17T17:56:20.377221Z","shell.execute_reply":"2025-10-17T17:56:20.390518Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_100/4082456899.py:34: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler(enabled=USE_AMP)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"start = time.time()\nprint(f\"Starting baseline training for {BASELINE_EPOCHS} epochs on {device}.\")\nprint(f\"Class counts (fake=0, real=1): {cls_counts} | class weights: {cls_weights}\")\n\nhistory = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n\nfor epoch in range(1, BASELINE_EPOCHS + 1):\n    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, scaler)\n    va_loss, va_acc = evaluate(model, test_loader)\n    scheduler.step()\n\n    history[\"train_loss\"].append(tr_loss)\n    history[\"train_acc\"].append(tr_acc)\n    history[\"val_loss\"].append(va_loss)\n    history[\"val_acc\"].append(va_acc)\n\n    lr_now = scheduler.get_last_lr()[0]\n    print(f\"[Epoch {epoch:02d}/{BASELINE_EPOCHS}] \"\n          f\"lr={lr_now:.2e} | train loss={tr_loss:.4f} acc={tr_acc:.4f} | \"\n          f\"val loss={va_loss:.4f} acc={va_acc:.4f}\")\n\nimport os\nCKPT_PATH = \"/kaggle/working/baseline_model.pth\"\nstate = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\nos.makedirs(os.path.dirname(CKPT_PATH), exist_ok=True)\ntorch.save(state, CKPT_PATH)\nprint(f\"\\nBaseline model checkpoint saved to: {CKPT_PATH}\")\nprint(f\"Total training time: {(time.time()-start)/60:.1f} min\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:56:20.392226Z","iopub.execute_input":"2025-10-17T17:56:20.392434Z","iopub.status.idle":"2025-10-17T18:11:02.198801Z","shell.execute_reply.started":"2025-10-17T17:56:20.392420Z","shell.execute_reply":"2025-10-17T18:11:02.197846Z"}},"outputs":[{"name":"stdout","text":"Starting baseline training for 8 epochs on cuda.\nClass counts (fake=0, real=1): [771, 1000] | class weights: [1.1485084306095978, 0.8855]\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_100/4082456899.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=USE_AMP):\n/tmp/ipykernel_100/4082456899.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=USE_AMP):\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 01/8] lr=2.89e-04 | train loss=0.6715 acc=0.6166 | val loss=0.6788 acc=0.6109\n[Epoch 02/8] lr=2.56e-04 | train loss=0.4883 acc=0.7792 | val loss=0.7136 acc=0.6257\n[Epoch 03/8] lr=2.07e-04 | train loss=0.3806 acc=0.8628 | val loss=0.8236 acc=0.5834\n[Epoch 04/8] lr=1.50e-04 | train loss=0.2693 acc=0.9317 | val loss=0.9704 acc=0.6146\n[Epoch 05/8] lr=9.26e-05 | train loss=0.2538 acc=0.9407 | val loss=0.9916 acc=0.6273\n[Epoch 06/8] lr=4.39e-05 | train loss=0.2254 acc=0.9571 | val loss=0.9782 acc=0.6321\n[Epoch 07/8] lr=1.14e-05 | train loss=0.1717 acc=0.9757 | val loss=1.3172 acc=0.5961\n[Epoch 08/8] lr=0.00e+00 | train loss=0.1570 acc=0.9842 | val loss=1.1612 acc=0.6236\n\nBaseline model checkpoint saved to: /kaggle/working/baseline_model.pth\nTotal training time: 14.7 min\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n_IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32, device=device)\n_IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32, device=device)\n\ndef _norm_bounds():\n    x_min = (0.0 - _IMAGENET_MEAN) / _IMAGENET_STD\n    x_max = (1.0 - _IMAGENET_MEAN) / _IMAGENET_STD\n    return x_min.view(1,3,1,1), x_max.view(1,3,1,1)\n\ndef _scale_eps_alpha(epsilon: float, alpha: float):\n    eps = (torch.ones(3, device=device) * epsilon) / _IMAGENET_STD\n    alp = (torch.ones(3, device=device) * alpha) / _IMAGENET_STD\n    return eps.view(1,3,1,1), alp.view(1,3,1,1)\n\n@torch.no_grad()\ndef _clip_normed(x, x_min, x_max):\n    return torch.max(torch.min(x, x_max), x_min)\n\ndef pgd_attack(\n    model: nn.Module,\n    images: torch.Tensor,\n    labels: torch.Tensor,\n    epsilon: float = 8/255,  \n    alpha: float = 2/255,   \n    iters: int = 10,\n    random_start: bool = True,\n) -> torch.Tensor:\n   \n    model.eval()  \n    x = images.detach().clone()\n    y = labels.detach().clone().to(device)\n\n    x_min, x_max = _norm_bounds()\n    eps_t, alpha_t = _scale_eps_alpha(epsilon, alpha)\n\n    if random_start:\n        x = x + torch.empty_like(x).uniform_(-1.0, 1.0) * eps_t\n        x = _clip_normed(x, x_min, x_max)\n\n    x_adv = x.clone().requires_grad_(True)\n    ce = nn.CrossEntropyLoss()\n\n    for _ in range(iters):\n        logits = model(x_adv)\n        loss = ce(logits, y)\n        grad = torch.autograd.grad(loss, x_adv, retain_graph=False, create_graph=False)[0]\n\n        x_adv = x_adv.detach() + alpha_t * grad.sign()\n        delta = torch.clamp(x_adv - x, min=-eps_t, max=eps_t)\n        x_adv = _clip_normed(x + delta, x_min, x_max).detach()\n        x_adv.requires_grad_(True)\n\n    return x_adv.detach()\n\ndef evaluate_accuracy(model: nn.Module, loader, attack: bool = False, **pgd_kwargs):\n    model.eval()\n    n_correct = 0\n    n_total = 0\n\n    for xb, yb in loader:\n        xb = xb.to(device, non_blocking=True)\n        yb = yb.to(device, non_blocking=True)\n\n        if attack:\n            # PGD needs gradients!\n            with torch.enable_grad():\n                xb = pgd_attack(model, xb, yb, **pgd_kwargs)\n\n        # No gradients needed for accuracy eval\n        with torch.no_grad():\n            logits = model(xb)\n            preds = logits.argmax(dim=1)\n\n        n_correct += (preds == yb).sum().item()\n        n_total += yb.numel()\n\n    return n_correct / max(1, n_total)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:11:02.202100Z","iopub.execute_input":"2025-10-17T18:11:02.202674Z","iopub.status.idle":"2025-10-17T18:11:02.214921Z","shell.execute_reply.started":"2025-10-17T18:11:02.202648Z","shell.execute_reply":"2025-10-17T18:11:02.214321Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True\n\nCKPT_PATH = \"/kaggle/working/baseline_model.pth\"\n\ndef _strip_module(sd):\n    return { (k[7:] if k.startswith(\"module.\") else k): v for k, v in sd.items() }\n\neval_model = EfficientViT().to(device)\n\nckpt = torch.load(CKPT_PATH, map_location=device)\ntry:\n    eval_model.load_state_dict(ckpt, strict=True)\nexcept RuntimeError:\n    eval_model.load_state_dict(_strip_module(ckpt), strict=True)\n\neval_model.eval()\n\nPGD_CFG = dict(epsilon=8/255, alpha=2/255, iters=10, random_start=True)\n\nclean_acc = evaluate_accuracy(eval_model, test_loader, attack=False)\npgd_acc   = evaluate_accuracy(eval_model, test_loader, attack=True, **PGD_CFG)\nprint(f\"[Celeb-DF v2] Clean Acc: {clean_acc:.4f} | PGD Acc: {pgd_acc:.4f} (Δ={clean_acc-pgd_acc:.4f})\")\n\nclean_acc_ff = evaluate_accuracy(eval_model, train_loader, attack=False)\npgd_acc_ff   = evaluate_accuracy(eval_model, train_loader, attack=True, **PGD_CFG)\nprint(f\"[FF++ c23]   Clean Acc: {clean_acc_ff:.4f} | PGD Acc: {pgd_acc_ff:.4f} (Δ={clean_acc_ff-pgd_acc_ff:.4f})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:11:02.215662Z","iopub.execute_input":"2025-10-17T18:11:02.215908Z","iopub.status.idle":"2025-10-17T18:16:37.895261Z","shell.execute_reply.started":"2025-10-17T18:11:02.215887Z","shell.execute_reply":"2025-10-17T18:16:37.894404Z"}},"outputs":[{"name":"stdout","text":"[Celeb-DF v2] Clean Acc: 0.6241 | PGD Acc: 0.4807 (Δ=0.1435)\n[FF++ c23]   Clean Acc: 0.9994 | PGD Acc: 0.4704 (Δ=0.5291)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"xb, yb = next(iter(test_loader))\nxb = xb.to(device); yb = yb.to(device)\n\nwith torch.no_grad():\n    clean_pred = eval_model(xb).argmax(1)\n\nxb_adv = pgd_attack(eval_model, xb, yb, **PGD_CFG)\nwith torch.no_grad():\n    adv_pred = eval_model(xb_adv).argmax(1)\n\nflip_rate = (clean_pred != adv_pred).float().mean().item()\nprint(f\"Label-change rate on a single test batch after PGD: {flip_rate:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:16:37.896310Z","iopub.execute_input":"2025-10-17T18:16:37.896558Z","iopub.status.idle":"2025-10-17T18:16:41.383556Z","shell.execute_reply.started":"2025-10-17T18:16:37.896535Z","shell.execute_reply":"2025-10-17T18:16:41.382674Z"}},"outputs":[{"name":"stdout","text":"Label-change rate on a single test batch after PGD: 0.500\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass _DoubleConv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.GELU(),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.GELU(),\n        )\n\n    def forward(self, x): return self.block(x)\n\nclass _Down(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.pool = nn.MaxPool2d(2)\n        self.conv = _DoubleConv(in_ch, out_ch)\n\n    def forward(self, x): return self.conv(self.pool(x))\n\nclass _Up(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n        self.conv = _DoubleConv(in_ch, out_ch) \n\n    def forward(self, x, skip):\n        x = self.up(x)\n        dh = skip.size(-2) - x.size(-2)\n        dw = skip.size(-1) - x.size(-1)\n        if dh != 0 or dw != 0:\n            x = F.pad(x, (0, max(0, dw), 0, max(0, dh)))\n            x = x[:, :, :skip.size(-2), :skip.size(-1)]\n        x = torch.cat([x, skip], dim=1)\n        return self.conv(x)\n\nclass TinyUNetAttacker(nn.Module):\n   \n    def __init__(self, in_ch=3, base_ch=16, depth=3, epsilon=8/255.0):\n        super().__init__()\n        assert depth == 3, \"This tiny configuration is hard-coded for depth=3.\"\n\n        self.epsilon = float(epsilon)  \n        # Encoder\n        self.inc   = _DoubleConv(in_ch, base_ch)       \n        self.down1 = _Down(base_ch, base_ch * 2)       \n        self.down2 = _Down(base_ch * 2, base_ch * 4)\n        self.bot   = _DoubleConv(base_ch * 4, base_ch * 8)\n        self.up2   = _Up(base_ch * 8, base_ch * 4)      \n        self.up1   = _Up(base_ch * 4, base_ch * 2)      \n        self.up0   = _Up(base_ch * 2, base_ch)      \n        self.outc  = nn.Conv2d(base_ch, in_ch, kernel_size=3, padding=1)\n        self.tanh  = nn.Tanh()\n\n    def _eps_tensor(self, like: torch.Tensor):\n        eps_t, _ = _scale_eps_alpha(self.epsilon, self.epsilon)  \n        return eps_t.to(like.device, like.dtype)  \n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x1 = self.inc(x)        \n        x2 = self.down1(x1)       \n        x3 = self.down2(x2)       \n        xb = self.bot(x3)         \n\n        y2 = self.up2(xb, x3)    \n        y1 = self.up1(y2, x2)     \n        y0 = self.up0(y1, x1)     \n\n        raw = self.outc(y0)      \n        delta = self.tanh(raw) * self._eps_tensor(raw)\n        return delta\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:16:41.384624Z","iopub.execute_input":"2025-10-17T18:16:41.385526Z","iopub.status.idle":"2025-10-17T18:16:41.399235Z","shell.execute_reply.started":"2025-10-17T18:16:41.385498Z","shell.execute_reply":"2025-10-17T18:16:41.398445Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"attacker = TinyUNetAttacker(epsilon=8/255.0).to(device)\nattacker.eval()  \n\nxb, yb = next(iter(test_loader))\nxb = xb.to(device)\n\nwith torch.no_grad():\n    delta = attacker(xb)                \n    x_adv = xb + delta                 \n    x_min, x_max = _norm_bounds()\n    x_adv = _clip_normed(x_adv, x_min, x_max)\n\nprint(\"Input   :\", tuple(xb.shape))\nprint(\"Delta   :\", tuple(delta.shape), \n      \"| min/max (per-batch):\", float(delta.min().item()), \"/\", float(delta.max().item()))\nprint(\"x_adv   :\", tuple(x_adv.shape),\n      \"| within bounds:\",\n      bool((x_adv <= x_max + 1e-6).all().item() and (x_adv >= x_min - 1e-6).all().item()))\n\ndef _num_params(m): return sum(p.numel() for p in m.parameters() if p.requires_grad)\nprint(\"TinyUNetAttacker params:\", f\"{_num_params(attacker):,}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:16:41.400006Z","iopub.execute_input":"2025-10-17T18:16:41.400355Z","iopub.status.idle":"2025-10-17T18:16:42.239955Z","shell.execute_reply.started":"2025-10-17T18:16:41.400332Z","shell.execute_reply":"2025-10-17T18:16:42.239149Z"}},"outputs":[{"name":"stdout","text":"Input   : (32, 3, 224, 224)\nDelta   : (32, 3, 224, 224) | min/max (per-batch): -0.008672297932207584 / 0.010881473310291767\nx_adv   : (32, 3, 224, 224) | within bounds: True\nTinyUNetAttacker params: 483,155\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"@torch.no_grad()\ndef unet_attack(attacker: nn.Module, images: torch.Tensor, epsilon: float = 8/255.0) -> torch.Tensor:\n    attacker.eval()\n    if hasattr(attacker, \"epsilon\") and abs(attacker.epsilon - float(epsilon)) > 1e-12:\n        attacker.epsilon = float(epsilon)\n    delta = attacker(images)\n    x_min, x_max = _norm_bounds()\n    return _clip_normed(images + delta, x_min, x_max)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:16:42.241039Z","iopub.execute_input":"2025-10-17T18:16:42.241273Z","iopub.status.idle":"2025-10-17T18:16:42.246670Z","shell.execute_reply.started":"2025-10-17T18:16:42.241252Z","shell.execute_reply":"2025-10-17T18:16:42.245959Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef total_variation_loss(delta: torch.Tensor, reduction: str = \"mean\") -> torch.Tensor:\n    dh = delta[:, :, 1:, :] - delta[:, :, :-1, :]\n    dw = delta[:, :, :, 1:] - delta[:, :, :, :-1]\n    tv = dh.abs() + dw.abs()\n    if reduction == \"sum\":\n        return tv.sum()\n    return tv.mean() \n\n_FREQ_MASK_CACHE: dict[tuple, torch.Tensor] = {}\n\ndef _low_freq_mask(h: int, w: int, radius_frac: float, device, dtype) -> torch.Tensor:\n    key = (h, w, float(radius_frac), str(device), str(dtype))\n    m = _FREQ_MASK_CACHE.get(key)\n    if m is not None and m.device == device and m.dtype == dtype:\n        return m\n\n    yy, xx = torch.meshgrid(\n        torch.arange(h, device=device, dtype=torch.float32),\n        torch.arange(w, device=device, dtype=torch.float32),\n        indexing=\"ij\",\n    )\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    dist = torch.sqrt((yy - cy) ** 2 + (xx - cx) ** 2)\n    radius = max(1.0, radius_frac * float(min(h, w)))\n    mask = (dist <= radius).to(dtype).unsqueeze(0).unsqueeze(0)  \n    _FREQ_MASK_CACHE[key] = mask\n    return mask\n\ndef frequency_loss(\n    delta: torch.Tensor,\n    radius_frac: float = 0.125,   \n    power: int = 2,              \n    norm: str = \"ortho\",\n) -> torch.Tensor:\n    B, C, H, W = delta.shape\n    f = torch.fft.fft2(delta, norm=norm)\n    f = torch.fft.fftshift(f, dim=(-2, -1))\n\n    mag = f.abs()\n    if power == 2:\n        mag = mag * mag\n\n    mask = _low_freq_mask(H, W, radius_frac, device=delta.device, dtype=delta.dtype)\n    # Broadcast over (B,C,H,W)\n    loss = (mag * mask).mean()\n    return loss\n\ndef realism_loss(\n    delta: torch.Tensor,\n    lambda_tv: float = 1e-4,\n    lambda_freq: float = 1e-4,\n    radius_frac: float = 0.125,\n) -> tuple[torch.Tensor, dict]:\n    l_tv = total_variation_loss(delta)\n    l_fq = frequency_loss(delta, radius_frac=radius_frac)\n    total = lambda_tv * l_tv + lambda_freq * l_fq\n    return total, {\"tv\": l_tv.detach(), \"freq\": l_fq.detach()}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:16:42.247391Z","iopub.execute_input":"2025-10-17T18:16:42.247668Z","iopub.status.idle":"2025-10-17T18:16:42.263802Z","shell.execute_reply.started":"2025-10-17T18:16:42.247650Z","shell.execute_reply":"2025-10-17T18:16:42.263034Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def total_variation_loss(delta: torch.Tensor, reduction: str = \"mean\", beta: float = 1.0) -> torch.Tensor:\n      dh = delta[:, :, 1:, :] - delta[:, :, :-1, :]\n    dw = delta[:, :, :, 1:] - delta[:, :, :, :-1]\n\n    if beta != 1.0:\n        dh = dh.abs().pow(beta)\n        dw = dw.abs().pow(beta)\n    else:\n        dh = dh.abs()\n        dw = dw.abs()\n\n    if reduction == \"sum\":\n        return dh.sum() + dw.sum()\n    return dh.mean() + dw.mean()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:16:42.264532Z","iopub.execute_input":"2025-10-17T18:16:42.264795Z","iopub.status.idle":"2025-10-17T18:16:42.276988Z","shell.execute_reply.started":"2025-10-17T18:16:42.264778Z","shell.execute_reply":"2025-10-17T18:16:42.276297Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"xb, yb = next(iter(test_loader))\nxb = xb.to(device)\nattacker.eval()\nwith torch.no_grad():\n    delta_pred = attacker(xb)\n\ntv_val  = total_variation_loss(delta_pred).item()\nfq_val  = frequency_loss(delta_pred, radius_frac=0.125).item()\ncomb, parts = realism_loss(delta_pred, lambda_tv=1e-4, lambda_freq=1e-4, radius_frac=0.125)\n\nprint(f\"Predicted δ → TV: {tv_val:.6f} | Freq(low): {fq_val:.6f} | \"\n      f\"Combined (λ_tv=λ_fq=1e-4): {comb.item():.6f}\")\n\ndef _make_sine_delta(batch: int, ch: int, H: int, W: int, freq: int, amp: float = 0.01, device=\"cpu\"):\n    yy, xx = torch.meshgrid(\n        torch.arange(H, device=device, dtype=torch.float32),\n        torch.arange(W, device=device, dtype=torch.float32),\n        indexing=\"ij\"\n    )\n    wave = torch.sin(2.0 * torch.pi * freq * xx / float(W)).unsqueeze(0).unsqueeze(0)\n    wave = wave.expand(batch, ch, H, W) * amp\n    return wave\n\nB, C, H, W = xb.shape\ndelta_low  = _make_sine_delta(B, C, H, W, freq=2,  amp=0.01, device=device)\ndelta_high = _make_sine_delta(B, C, H, W, freq=32, amp=0.01, device=device)\n\nprint(\"Low-freq sine  → TV:\", float(total_variation_loss(delta_low)),\n      \"| Freq(low):\", float(frequency_loss(delta_low)))\nprint(\"High-freq sine → TV:\", float(total_variation_loss(delta_high)),\n      \"| Freq(low):\", float(frequency_loss(delta_high)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:16:42.277636Z","iopub.execute_input":"2025-10-17T18:16:42.277819Z","iopub.status.idle":"2025-10-17T18:16:43.406087Z","shell.execute_reply.started":"2025-10-17T18:16:42.277805Z","shell.execute_reply":"2025-10-17T18:16:43.404260Z"}},"outputs":[{"name":"stdout","text":"Predicted δ → TV: 0.000057 | Freq(low): 0.000055 | Combined (λ_tv=λ_fq=1e-4): 0.000000\nLow-freq sine  → TV: 0.0003562300989869982 | Freq(low): 4.999998418497853e-05\nHigh-freq sine → TV: 0.005560940597206354 | Freq(low): 3.718621545723168e-16\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import os, time\nimport torch\nimport torch.nn as nn\nfrom torch.cuda.amp import GradScaler, autocast\n\nos.environ.setdefault(\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\", \"python\")\nos.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\nos.environ.setdefault(\"TENSORBOARD_NO_TF\", \"1\")\nos.environ.setdefault(\"MPLCONFIGDIR\", \"/kaggle/working/mpl\")\nos.makedirs(os.environ[\"MPLCONFIGDIR\"], exist_ok=True)\n\ntry:\n    from torch.utils.tensorboard import SummaryWriter\n    _TB_IMPL = \"torch.utils.tensorboard\"\nexcept Exception as e:\n    print(\"Native TensorBoard import failed, falling back to tensorboardX. Reason:\", repr(e))\n    %pip -q install tensorboardX\n    from tensorboardX import SummaryWriter\n    _TB_IMPL = \"tensorboardX\"\n\nprint(\"Using SummaryWriter from:\", _TB_IMPL)\n\nROBUST_EPOCHS         = 8             \nLR_DETECTOR           = 2e-4\nLR_ATTACKER           = 1e-4\nWD_DETECTOR           = 1e-4\nWD_ATTACKER           = 1e-5\nMAX_NORM_DET          = 1.0\nMAX_NORM_ATT          = 1.0\nLAMBDA_TV             = 1e-4           \nLAMBDA_FREQ           = 1e-4\nFREQ_RADIUS_FRAC      = 0.125           \n\nPGD_KW = dict(epsilon=8/255, alpha=2/255, iters=10, random_start=True)\nif 'PGD_CFG' in globals() and isinstance(PGD_CFG, dict):\n    PGD_KW.update(PGD_CFG)\n\noptimizer_det = torch.optim.AdamW(\n    model.parameters(), lr=LR_DETECTOR, weight_decay=WD_DETECTOR\n)\noptimizer_att = torch.optim.AdamW(\n    attacker.parameters(), lr=LR_ATTACKER, weight_decay=WD_ATTACKER\n)\n\nUSE_AMP = torch.cuda.is_available() if 'USE_AMP' not in globals() else USE_AMP\ndet_scaler = GradScaler(enabled=USE_AMP)\natt_scaler = GradScaler(enabled=USE_AMP)\n\nTB_DIR = \"/kaggle/working/tensorboard/phase3_2\"\nos.makedirs(TB_DIR, exist_ok=True)\nwriter = SummaryWriter(log_dir=TB_DIR)\n\nprint(\"Robust training config:\",\n      {\"epochs\": ROBUST_EPOCHS, \"lr_det\": LR_DETECTOR, \"lr_att\": LR_ATTACKER,\n       \"lambda_tv\": LAMBDA_TV, \"lambda_freq\": LAMBDA_FREQ, \"pgd\": PGD_KW,\n       \"tb_impl\": _TB_IMPL, \"tb_dir\": TB_DIR})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef _batch_acc(logits, y):\n    return (logits.argmax(1) == y).float().mean().item()\n\ndef train_robust_epoch(epoch_idx: int):\n    model.train()\n    attacker.train()\n\n    sums = {\n        \"loss_clean\": 0.0,\n        \"loss_pgd\": 0.0,\n        \"loss_unet_det\": 0.0,\n        \"loss_att_total\": 0.0,\n        \"loss_att_tv\": 0.0,\n        \"loss_att_freq\": 0.0,\n        \"acc_clean\": 0.0,\n        \"acc_pgd\": 0.0,\n        \"acc_unet\": 0.0,\n        \"n\": 0,\n    }\n\n    x_min, x_max = _norm_bounds()  \n\n    for xb, yb in train_loader:\n        xb = xb.to(device, non_blocking=True)\n        yb = yb.to(device, non_blocking=True)\n        bsz = xb.size(0)\n\n        optimizer_det.zero_grad(set_to_none=True)\n        with autocast(enabled=USE_AMP):\n            logits_clean = model(xb)\n            loss_clean   = criterion(logits_clean, yb)\n\n        with torch.enable_grad():\n            xb_pgd = pgd_attack(model, xb, yb, **PGD_KW)\n        with autocast(enabled=USE_AMP):\n            logits_pgd = model(xb_pgd)\n            loss_pgd   = criterion(logits_pgd, yb)\n\n        optimizer_att.zero_grad(set_to_none=True)\n\n        for p in model.parameters():\n            p.requires_grad_(False)\n\n        with autocast(enabled=USE_AMP):\n            delta = attacker(xb)                      \n            xb_un = _clip_normed(xb + delta, x_min, x_max)\n            logits_un_for_att = model(xb_un)         \n            ce_for_att = criterion(logits_un_for_att, yb)\n            l_tv  = total_variation_loss(delta)\n            l_fq  = frequency_loss(delta, radius_frac=FREQ_RADIUS_FRAC)\n            loss_att_total = (-ce_for_att) + LAMBDA_TV * l_tv + LAMBDA_FREQ * l_fq\n\n        att_scaler.scale(loss_att_total).backward()\n        nn.utils.clip_grad_norm_(attacker.parameters(), MAX_NORM_ATT)\n        att_scaler.step(optimizer_att)\n        att_scaler.update()\n\n        # Unfreeze detector for its own update\n        for p in model.parameters():\n            p.requires_grad_(True)\n\n        with autocast(enabled=USE_AMP):\n            with torch.no_grad():\n                delta_det = attacker(xb)\n            xb_un = _clip_normed(xb + delta_det, x_min, x_max)\n            logits_un_det = model(xb_un)\n            loss_unet_det = criterion(logits_un_det, yb)\n\n            loss_total_det = loss_clean + loss_pgd + loss_unet_det\n\n        det_scaler.scale(loss_total_det).backward()\n        nn.utils.clip_grad_norm_(model.parameters(), MAX_NORM_DET)\n        det_scaler.step(optimizer_det)\n        det_scaler.update()\n\n        with torch.no_grad():\n            sums[\"loss_clean\"]     += loss_clean.item()     * bsz\n            sums[\"loss_pgd\"]       += loss_pgd.item()       * bsz\n            sums[\"loss_unet_det\"]  += loss_unet_det.item()  * bsz\n            sums[\"loss_att_total\"] += loss_att_total.item() * bsz\n            sums[\"loss_att_tv\"]    += l_tv.item()           * bsz\n            sums[\"loss_att_freq\"]  += l_fq.item()           * bsz\n            sums[\"acc_clean\"]      += _batch_acc(logits_clean, yb)   * bsz\n            sums[\"acc_pgd\"]        += _batch_acc(logits_pgd, yb)     * bsz\n            sums[\"acc_unet\"]       += _batch_acc(logits_un_det, yb)  * bsz\n            sums[\"n\"]              += bsz\n\n    n = max(1, sums[\"n\"])\n    logs = {k: (v / n) for k, v in sums.items() if k != \"n\"}\n    # TensorBoard\n    writer.add_scalar(\"train/loss_clean\",     logs[\"loss_clean\"],    epoch_idx)\n    writer.add_scalar(\"train/loss_pgd\",       logs[\"loss_pgd\"],      epoch_idx)\n    writer.add_scalar(\"train/loss_unet_det\",  logs[\"loss_unet_det\"], epoch_idx)\n    writer.add_scalar(\"train/loss_att_total\", logs[\"loss_att_total\"],epoch_idx)\n    writer.add_scalar(\"train/att_tv\",         logs[\"loss_att_tv\"],   epoch_idx)\n    writer.add_scalar(\"train/att_freq\",       logs[\"loss_att_freq\"], epoch_idx)\n    writer.add_scalar(\"train/acc_clean\",      logs[\"acc_clean\"],     epoch_idx)\n    writer.add_scalar(\"train/acc_pgd\",        logs[\"acc_pgd\"],       epoch_idx)\n    writer.add_scalar(\"train/acc_unet\",       logs[\"acc_unet\"],      epoch_idx)\n\n    return logs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:17:20.380783Z","iopub.execute_input":"2025-10-17T18:17:20.381380Z","iopub.status.idle":"2025-10-17T18:17:20.398037Z","shell.execute_reply.started":"2025-10-17T18:17:20.381356Z","shell.execute_reply":"2025-10-17T18:17:20.397085Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def frequency_loss(\n    delta: torch.Tensor,\n    radius_frac: float = 0.125,\n    power: int = 2,\n    norm: str = \"ortho\",\n) -> torch.Tensor:\n    B, C, H, W = delta.shape\n    x = delta.float() \n\n    f = torch.fft.fft2(x, norm=norm)\n    f = torch.fft.fftshift(f, dim=(-2, -1))\n\n    mag = f.abs()\n    if power == 2:\n        mag = mag * mag\n\n    mask = _low_freq_mask(H, W, radius_frac, device=x.device, dtype=torch.float32)  \n    loss = (mag * mask).mean()\n    return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:17:20.398981Z","iopub.execute_input":"2025-10-17T18:17:20.399208Z","iopub.status.idle":"2025-10-17T18:17:20.418739Z","shell.execute_reply.started":"2025-10-17T18:17:20.399193Z","shell.execute_reply":"2025-10-17T18:17:20.417730Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"start = time.time()\nprint(f\"Starting Phase 3.2 robust training for {ROBUST_EPOCHS} epochs on {device}.\")\n\nhistory_robust = []\nfor ep in range(1, ROBUST_EPOCHS + 1):\n    logs = train_robust_epoch(ep)\n    history_robust.append(logs)\n    print(f\"[Robust Ep {ep:02d}/{ROBUST_EPOCHS}] \"\n          f\"clean loss {logs['loss_clean']:.4f} acc {logs['acc_clean']:.3f} | \"\n          f\"pgd loss {logs['loss_pgd']:.4f} acc {logs['acc_pgd']:.3f} | \"\n          f\"unet loss {logs['loss_unet_det']:.4f} acc {logs['acc_unet']:.3f} | \"\n          f\"att(L): {logs['loss_att_total']:.5f} (tv {logs['loss_att_tv']:.5f}, fq {logs['loss_att_freq']:.5f})\")\n\nROBUST_CKPT = \"/kaggle/working/robust_model.pth\"\nstate_dict = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\ntorch.save(state_dict, ROBUST_CKPT)\nwriter.flush(); writer.close()\n\nprint(f\"\\nSaved robust checkpoint to: {ROBUST_CKPT}\")\nprint(f\"Phase 3.2 total time: {(time.time()-start)/60:.1f} min\")\nprint(f\"TensorBoard logs at: {TB_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:17:20.419662Z","iopub.execute_input":"2025-10-17T18:17:20.419908Z","iopub.status.idle":"2025-10-17T18:36:16.198101Z","shell.execute_reply.started":"2025-10-17T18:17:20.419890Z","shell.execute_reply":"2025-10-17T18:36:16.197156Z"}},"outputs":[{"name":"stdout","text":"Starting Phase 3.2 robust training for 8 epochs on cuda.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_100/958576699.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=USE_AMP):\n/tmp/ipykernel_100/958576699.py:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=USE_AMP):\n/tmp/ipykernel_100/958576699.py:54: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=USE_AMP):\n/tmp/ipykernel_100/958576699.py:73: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=USE_AMP):\n","output_type":"stream"},{"name":"stdout","text":"[Robust Ep 01/8] clean loss 0.1203 acc 0.999 | pgd loss nan acc 0.460 | unet loss 0.1954 acc 0.968 | att(L): -0.20297 (tv 0.01482, fq 0.00127)\n[Robust Ep 02/8] clean loss 0.1233 acc 0.998 | pgd loss nan acc 0.464 | unet loss 0.1239 acc 0.998 | att(L): -0.12411 (tv 0.00634, fq 0.00157)\n[Robust Ep 03/8] clean loss 0.1239 acc 0.998 | pgd loss nan acc 0.453 | unet loss 0.1223 acc 0.999 | att(L): -0.12230 (tv 0.00521, fq 0.00152)\n[Robust Ep 04/8] clean loss 0.1207 acc 0.999 | pgd loss nan acc 0.477 | unet loss 0.1206 acc 1.000 | att(L): -0.12056 (tv 0.00467, fq 0.00142)\n[Robust Ep 05/8] clean loss 0.1227 acc 0.998 | pgd loss nan acc 0.441 | unet loss 0.1214 acc 0.999 | att(L): -0.12147 (tv 0.00440, fq 0.00137)\n[Robust Ep 06/8] clean loss 0.1219 acc 0.999 | pgd loss nan acc 0.458 | unet loss 0.1222 acc 0.998 | att(L): -0.12209 (tv 0.00404, fq 0.00128)\n[Robust Ep 07/8] clean loss 0.1219 acc 0.999 | pgd loss nan acc 0.469 | unet loss 0.1216 acc 0.998 | att(L): -0.12161 (tv 0.00375, fq 0.00119)\n[Robust Ep 08/8] clean loss 0.1233 acc 0.998 | pgd loss nan acc 0.478 | unet loss 0.1218 acc 0.999 | att(L): -0.12176 (tv 0.00363, fq 0.00111)\n\nSaved robust checkpoint to: /kaggle/working/robust_model.pth\nPhase 3.2 total time: 18.9 min\nTensorBoard logs at: /kaggle/working/tensorboard/phase3_2\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate_unet_acc(model, loader, attacker):\n    model.eval(); attacker.eval()\n    n_correct, n_total = 0, 0\n    for xb, yb in loader:\n        xb = xb.to(device, non_blocking=True)\n        yb = yb.to(device, non_blocking=True)\n        xb_un = unet_attack(attacker, xb) \n        logits = model(xb_un)\n        n_correct += (logits.argmax(1) == yb).sum().item()\n        n_total   += yb.numel()\n    return n_correct / max(1, n_total)\n\nclean_acc_celeb = evaluate_accuracy(model, test_loader, attack=False)\npgd_acc_celeb   = evaluate_accuracy(model, test_loader, attack=True, **PGD_KW)\nunet_acc_celeb  = evaluate_unet_acc(model, test_loader, attacker)\n\nprint(f\"[Celeb-DF v2]  Clean Acc: {clean_acc_celeb:.4f} | \"\n      f\"PGD Acc: {pgd_acc_celeb:.4f} | U-Net Acc: {unet_acc_celeb:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:36:16.199077Z","iopub.execute_input":"2025-10-17T18:36:16.199338Z","iopub.status.idle":"2025-10-17T18:38:30.028355Z","shell.execute_reply.started":"2025-10-17T18:36:16.199317Z","shell.execute_reply":"2025-10-17T18:38:30.027534Z"}},"outputs":[{"name":"stdout","text":"[Celeb-DF v2]  Clean Acc: 0.6167 | PGD Acc: 0.4711 | U-Net Acc: 0.6067\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import os, uuid, json, shutil, tempfile\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport cv2\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nPGD_KW = dict(epsilon=8/255, alpha=2/255, iters=10, random_start=True) if 'PGD_KW' not in globals() else PGD_KW\n\n_IMAGENET_MEAN = _IMAGENET_MEAN if '_IMAGENET_MEAN' in globals() else torch.tensor([0.485, 0.456, 0.406], device=device, dtype=torch.float32)\n_IMAGENET_STD  = _IMAGENET_STD  if '_IMAGENET_STD'  in globals() else torch.tensor([0.229, 0.224, 0.225], device=device, dtype=torch.float32)\n\ndef _denorm(x: torch.Tensor) -> torch.Tensor:\n    return (x * _IMAGENET_STD.view(1,3,1,1)) + _IMAGENET_MEAN.view(1,3,1,1)\n\ndef _renorm(x01: torch.Tensor) -> torch.Tensor:\n    return (x01 - _IMAGENET_MEAN.view(1,3,1,1)) / _IMAGENET_STD.view(1,3,1,1)\n\ndef _to_uint8(x_norm: torch.Tensor) -> np.ndarray:\n    x01 = _denorm(x_norm).clamp(0,1)\n    return (x01.detach().cpu().permute(0,2,3,1).numpy() * 255.0).round().astype(np.uint8)\n\ndef _from_uint8(nhwc: np.ndarray) -> torch.Tensor:\n    x = torch.tensor(nhwc, device=device, dtype=torch.float32) / 255.0\n    x = x.permute(0,3,1,2)\n    return _renorm(x)\n\ndef jpeg_compress_batch(x_norm: torch.Tensor, quality: int = 50) -> torch.Tensor:\n    x_uint8 = _to_uint8(x_norm)\n    out = []\n    for img in x_uint8:\n        ok, buf = cv2.imencode(\".jpg\", cv2.cvtColor(img, cv2.COLOR_RGB2BGR),\n                               [int(cv2.IMWRITE_JPEG_QUALITY), int(quality)])\n        if not ok:\n            out.append(img)\n            continue\n        dec = cv2.imdecode(buf, cv2.IMREAD_COLOR)\n        dec = cv2.cvtColor(dec, cv2.COLOR_BGR2RGB)\n        out.append(dec)\n    return _from_uint8(np.stack(out, 0))\n\ndef h264_like_compress_batch(x_norm: torch.Tensor) -> torch.Tensor:\n    x_uint8 = _to_uint8(x_norm)\n    B, H, W, _ = x_uint8.shape\n    out = []\n    tmp = tempfile.mkdtemp(prefix=\"h264like_\")\n    try:\n        for img in x_uint8:\n            p = os.path.join(tmp, f\"{uuid.uuid4().hex}.mp4\")\n            fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n            vw = cv2.VideoWriter(p, fourcc, 1.0, (W, H))\n            if not vw.isOpened():\n                p = p.replace(\".mp4\", \".avi\")\n                vw = cv2.VideoWriter(p, cv2.VideoWriter_fourcc(*\"XVID\"), 1.0, (W, H))\n            vw.write(cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n            vw.release()\n\n            cap = cv2.VideoCapture(p)\n            ok, frame = cap.read()\n            cap.release()\n            if ok and frame is not None:\n                out.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n            else:\n                out.append(img)\n    finally:\n        shutil.rmtree(tmp, ignore_errors=True)\n    return _from_uint8(np.stack(out, 0))\n\nCKPT_BASELINE = \"/kaggle/working/baseline_model.pth\"\nCKPT_ROBUST   = \"/kaggle/working/robust_model.pth\"\n\ndef _strip_module(sd):\n    return { (k[7:] if k.startswith(\"module.\") else k): v for k, v in sd.items() }\n\ndef load_eval_model(ckpt_path: str) -> torch.nn.Module:\n    m = EfficientViT().to(device) \n    sd = torch.load(ckpt_path, map_location=device)\n    try:\n        m.load_state_dict(sd, strict=True)\n    except RuntimeError:\n        m.load_state_dict(_strip_module(sd), strict=True)\n    m.eval()\n    return m\n\neval_baseline = load_eval_model(CKPT_BASELINE)\neval_robust   = load_eval_model(CKPT_ROBUST)\nprint(\"Loaded:\", os.path.basename(CKPT_BASELINE), \"and\", os.path.basename(CKPT_ROBUST))\n\nif 'attacker' not in globals() or not isinstance(attacker, torch.nn.Module):\n    attacker = TinyUNetAttacker(epsilon=8/255.0).to(device)\nattacker.eval()\n\ndef _fake_index_from_loader(loader) -> int:\n    mapping = getattr(loader.dataset, \"class_to_idx\", {\"fake\": 1})\n    return int(mapping.get(\"fake\", 1))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:38:30.029391Z","iopub.execute_input":"2025-10-17T18:38:30.029698Z","iopub.status.idle":"2025-10-17T18:38:31.970099Z","shell.execute_reply.started":"2025-10-17T18:38:30.029675Z","shell.execute_reply":"2025-10-17T18:38:31.969302Z"}},"outputs":[{"name":"stdout","text":"Loaded: baseline_model.pth and robust_model.pth\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import numpy as np\n\ndef roc_curve_np(labels: np.ndarray, scores: np.ndarray):\n\n    labels = labels.astype(np.int32)\n    scores = scores.astype(np.float64)\n\n    # Sort by score descending (stable)\n    order = np.argsort(-scores, kind=\"mergesort\")\n    y = labels[order]\n    s = scores[order]\n\n    distinct = np.where(np.diff(s))[0]\n    thr_idx = np.r_[distinct, y.size - 1]\n\n    tps = np.cumsum(y == 1)[thr_idx]\n    fps = (1 + thr_idx) - tps\n\n    P = max(1, int(labels.sum()))\n    N = max(1, int(labels.size - labels.sum()))\n    tpr = tps / P\n    fpr = fps / N\n\n    tpr = np.r_[0.0, tpr]\n    fpr = np.r_[0.0, fpr]\n    thresholds = np.r_[np.inf, s[thr_idx]]\n    return fpr, tpr, thresholds\n\ndef roc_auc_score_np(labels: np.ndarray, scores: np.ndarray) -> float:\n    fpr, tpr, _ = roc_curve_np(labels, scores)\n    return float(np.trapz(tpr, fpr))\n\ndef tpr_at_fpr_np(labels: np.ndarray, scores: np.ndarray, target_fpr=0.01) -> float:\n    fpr, tpr, _ = roc_curve_np(labels, scores)\n    target_fpr = float(np.clip(target_fpr, 0.0, 1.0))\n    return float(np.interp(target_fpr, fpr, tpr))\n\n@torch.no_grad()\ndef _batch_argmax_acc(logits: torch.Tensor, y: torch.Tensor) -> float:\n    return (logits.argmax(1) == y).float().mean().item()\n\ndef evaluate_scenario(\n    model: torch.nn.Module,\n    loader,\n    attacker_module: torch.nn.Module | None = None,\n    attack: str | None = None,        \n    compression: str | None = None,    \n    jpeg_quality: int = 50,\n    pgd_kwargs: dict | None = None,\n) -> dict:\n    model.eval()\n    if attacker_module is not None:\n        attacker_module.eval()\n    if pgd_kwargs is None:\n        pgd_kwargs = PGD_KW\n\n    labels_all, scores_all = [], []\n    n_correct, n_total = 0, 0\n    fake_idx = _fake_index_from_loader(loader)\n\n    for xb, yb in loader:\n        xb = xb.to(device, non_blocking=True)\n        yb = yb.to(device, non_blocking=True)\n\n        # 1) attack (normalized)\n        if attack == \"pgd\":\n            with torch.enable_grad():\n                xb = pgd_attack(model, xb, yb, **pgd_kwargs)\n        elif attack == \"unet\":\n            if attacker_module is None:\n                raise ValueError(\"U-Net attack requested but attacker_module is None.\")\n            with torch.no_grad():\n                delta = attacker_module(xb)\n            x_min, x_max = _norm_bounds()\n            xb = _clip_normed(xb + delta, x_min, x_max)\n\n        # 2) compression\n        if compression == \"jpeg\":\n            xb = jpeg_compress_batch(xb, quality=jpeg_quality)\n        elif compression == \"h264\":\n            xb = h264_like_compress_batch(xb)\n\n        # 3) inference\n        with torch.no_grad():\n            logits = model(xb)\n            probs = torch.softmax(logits, dim=1)[:, fake_idx]\n        n_correct += (logits.argmax(1) == yb).sum().item()\n        n_total   += yb.numel()\n\n        labels_all.extend(yb.detach().cpu().numpy().tolist())\n        scores_all.extend(probs.detach().cpu().numpy().tolist())\n\n    labels_np = np.array(labels_all, dtype=np.int32)\n    scores_np = np.array(scores_all, dtype=np.float32)\n\n    # Make 'fake' the positive label (1)\n    if fake_idx == 0:\n        labels_np = 1 - labels_np\n\n    roc_auc = roc_auc_score_np(labels_np, scores_np)\n    tpr001  = tpr_at_fpr_np(labels_np, scores_np, target_fpr=0.01)\n    acc     = n_correct / max(1, n_total)\n    return {\"acc\": acc, \"roc_auc\": roc_auc, \"tpr@1%fpr\": tpr001}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:38:31.970899Z","iopub.execute_input":"2025-10-17T18:38:31.971219Z","iopub.status.idle":"2025-10-17T18:38:31.985728Z","shell.execute_reply.started":"2025-10-17T18:38:31.971193Z","shell.execute_reply":"2025-10-17T18:38:31.984834Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import pandas as pd\n\nSCENARIOS = [\n    (\"Clean\",                 dict(attack=None,    compression=None)),\n    (\"JPEG(50)\",              dict(attack=None,    compression=\"jpeg\", jpeg_quality=50)),\n    (\"H264-like\",             dict(attack=None,    compression=\"h264\")),\n    (\"PGD\",                   dict(attack=\"pgd\",   compression=None,   pgd_kwargs=PGD_KW)),\n    (\"U-Net\",                 dict(attack=\"unet\",  compression=None)),\n    (\"U-Net + JPEG(50)\",      dict(attack=\"unet\",  compression=\"jpeg\", jpeg_quality=50)),\n    (\"U-Net + H264-like\",     dict(attack=\"unet\",  compression=\"h264\")),\n]\n\ndef run_all(model, loader, loader_name: str, attacker_module=None):\n    rows = []\n    for name, kw in SCENARIOS:\n        res = evaluate_scenario(model, loader, attacker_module=attacker_module, **kw)\n        rows.append({\"dataset\": loader_name, \"scenario\": name,\n                     \"acc\": res[\"acc\"], \"roc_auc\": res[\"roc_auc\"], \"tpr@1%fpr\": res[\"tpr@1%fpr\"]})\n        print(f\"[{loader_name:12}] {name:16} | acc {res['acc']:.3f} | AUC {res['roc_auc']:.3f} | TPR@0.01 {res['tpr@1%fpr']:.3f}\")\n    return pd.DataFrame(rows)\n\nprint(\"\\n=== Evaluating BASELINE model ===\")\ndf_base_celeb = run_all(eval_baseline, test_loader,  \"Celeb-DF v2\", attacker_module=attacker)\ndf_base_ffpp  = run_all(eval_baseline, train_loader, \"FF++ c23\",   attacker_module=attacker)\n\nprint(\"\\n=== Evaluating ROBUST model ===\")\ndf_rob_celeb = run_all(eval_robust,   test_loader,  \"Celeb-DF v2\", attacker_module=attacker)\ndf_rob_ffpp  = run_all(eval_robust,   train_loader, \"FF++ c23\",    attacker_module=attacker)\n\n# Save + pivot\ndf_all = pd.concat([\n    df_base_celeb.assign(model=\"baseline\"),\n    df_base_ffpp.assign(model=\"baseline\"),\n    df_rob_celeb.assign(model=\"robust\"),\n    df_rob_ffpp.assign(model=\"robust\"),\n], ignore_index=True)\n\ncsv_path  = \"/kaggle/working/phase4_1_eval_results.csv\"\njson_path = \"/kaggle/working/phase4_1_eval_results.json\"\ndf_all.to_csv(csv_path, index=False)\nwith open(json_path, \"w\") as f:\n    json.dump(df_all.to_dict(orient=\"records\"), f, indent=2)\nprint(f\"\\nSaved results to:\\n- {csv_path}\\n- {json_path}\")\n\npivot = df_all.pivot_table(index=[\"dataset\",\"scenario\"], columns=\"model\",\n                           values=[\"acc\",\"roc_auc\",\"tpr@1%fpr\"]).sort_index()\npivot\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:38:31.986501Z","iopub.execute_input":"2025-10-17T18:38:31.986828Z","iopub.status.idle":"2025-10-17T18:54:37.923925Z","shell.execute_reply.started":"2025-10-17T18:38:31.986811Z","shell.execute_reply":"2025-10-17T18:54:37.923218Z"}},"outputs":[{"name":"stdout","text":"\n=== Evaluating BASELINE model ===\n[Celeb-DF v2 ] Clean            | acc 0.624 | AUC 0.678 | TPR@0.01 0.014\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_100/3948412121.py:41: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n  return float(np.trapz(tpr, fpr))\n","output_type":"stream"},{"name":"stdout","text":"[Celeb-DF v2 ] JPEG(50)         | acc 0.625 | AUC 0.672 | TPR@0.01 0.023\n[Celeb-DF v2 ] H264-like        | acc 0.636 | AUC 0.693 | TPR@0.01 0.022\n[Celeb-DF v2 ] PGD              | acc 0.467 | AUC 0.460 | TPR@0.01 0.001\n[Celeb-DF v2 ] U-Net            | acc 0.619 | AUC 0.675 | TPR@0.01 0.015\n[Celeb-DF v2 ] U-Net + JPEG(50) | acc 0.625 | AUC 0.675 | TPR@0.01 0.020\n[Celeb-DF v2 ] U-Net + H264-like | acc 0.644 | AUC 0.690 | TPR@0.01 0.025\n[FF++ c23    ] Clean            | acc 0.999 | AUC 1.000 | TPR@0.01 1.000\n[FF++ c23    ] JPEG(50)         | acc 0.851 | AUC 0.922 | TPR@0.01 0.344\n[FF++ c23    ] H264-like        | acc 0.912 | AUC 0.965 | TPR@0.01 0.641\n[FF++ c23    ] PGD              | acc 0.511 | AUC 0.465 | TPR@0.01 0.003\n[FF++ c23    ] U-Net            | acc 0.999 | AUC 1.000 | TPR@0.01 1.000\n[FF++ c23    ] U-Net + JPEG(50) | acc 0.859 | AUC 0.924 | TPR@0.01 0.340\n[FF++ c23    ] U-Net + H264-like | acc 0.906 | AUC 0.966 | TPR@0.01 0.626\n\n=== Evaluating ROBUST model ===\n[Celeb-DF v2 ] Clean            | acc 0.617 | AUC 0.676 | TPR@0.01 0.012\n[Celeb-DF v2 ] JPEG(50)         | acc 0.641 | AUC 0.681 | TPR@0.01 0.023\n[Celeb-DF v2 ] H264-like        | acc 0.661 | AUC 0.703 | TPR@0.01 0.024\n[Celeb-DF v2 ] PGD              | acc 0.476 | AUC 0.483 | TPR@0.01 0.001\n[Celeb-DF v2 ] U-Net            | acc 0.607 | AUC 0.674 | TPR@0.01 0.015\n[Celeb-DF v2 ] U-Net + JPEG(50) | acc 0.647 | AUC 0.685 | TPR@0.01 0.029\n[Celeb-DF v2 ] U-Net + H264-like | acc 0.654 | AUC 0.699 | TPR@0.01 0.030\n[FF++ c23    ] Clean            | acc 0.998 | AUC 1.000 | TPR@0.01 1.000\n[FF++ c23    ] JPEG(50)         | acc 0.836 | AUC 0.916 | TPR@0.01 0.387\n[FF++ c23    ] H264-like        | acc 0.912 | AUC 0.962 | TPR@0.01 0.671\n[FF++ c23    ] PGD              | acc 0.493 | AUC 0.449 | TPR@0.01 0.000\n[FF++ c23    ] U-Net            | acc 0.999 | AUC 1.000 | TPR@0.01 1.000\n[FF++ c23    ] U-Net + JPEG(50) | acc 0.844 | AUC 0.918 | TPR@0.01 0.379\n[FF++ c23    ] U-Net + H264-like | acc 0.903 | AUC 0.963 | TPR@0.01 0.651\n\nSaved results to:\n- /kaggle/working/phase4_1_eval_results.csv\n- /kaggle/working/phase4_1_eval_results.json\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"                                    acc             roc_auc            \\\nmodel                          baseline    robust  baseline    robust   \ndataset     scenario                                                    \nCeleb-DF v2 Clean              0.624140  0.616728  0.678174  0.675695   \n            H264-like          0.636316  0.661196  0.693363  0.702983   \n            JPEG(50)           0.624669  0.640551  0.671669  0.681253   \n            PGD                0.467443  0.476443  0.459703  0.483069   \n            U-Net              0.618846  0.606670  0.675271  0.674021   \n            U-Net + H264-like  0.643727  0.654314  0.690201  0.698939   \n            U-Net + JPEG(50)   0.625199  0.646903  0.674873  0.685051   \nFF++ c23    Clean              0.999435  0.998306  0.999997  0.999999   \n            H264-like          0.912479  0.911914  0.965480  0.962390   \n            JPEG(50)           0.850932  0.835686  0.922041  0.916370   \n            PGD                0.511011  0.492942  0.465106  0.449477   \n            U-Net              0.998871  0.998871  0.999997  0.999997   \n            U-Net + H264-like  0.905703  0.902880  0.965967  0.962840   \n            U-Net + JPEG(50)   0.859401  0.844156  0.923581  0.917866   \n\n                              tpr@1%fpr            \nmodel                          baseline    robust  \ndataset     scenario                               \nCeleb-DF v2 Clean              0.014000  0.012000  \n            H264-like          0.022000  0.023890  \n            JPEG(50)           0.023000  0.023000  \n            PGD                0.001000  0.001000  \n            U-Net              0.015000  0.015000  \n            U-Net + H264-like  0.025000  0.030000  \n            U-Net + JPEG(50)   0.020000  0.029000  \nFF++ c23    Clean              1.000000  1.000000  \n            H264-like          0.640726  0.670558  \n            JPEG(50)           0.343709  0.386511  \n            PGD                0.002594  0.000000  \n            U-Net              1.000000  1.000000  \n            U-Net + H264-like  0.626459  0.651102  \n            U-Net + JPEG(50)   0.339818  0.378729  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th></th>\n      <th colspan=\"2\" halign=\"left\">acc</th>\n      <th colspan=\"2\" halign=\"left\">roc_auc</th>\n      <th colspan=\"2\" halign=\"left\">tpr@1%fpr</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>model</th>\n      <th>baseline</th>\n      <th>robust</th>\n      <th>baseline</th>\n      <th>robust</th>\n      <th>baseline</th>\n      <th>robust</th>\n    </tr>\n    <tr>\n      <th>dataset</th>\n      <th>scenario</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"7\" valign=\"top\">Celeb-DF v2</th>\n      <th>Clean</th>\n      <td>0.624140</td>\n      <td>0.616728</td>\n      <td>0.678174</td>\n      <td>0.675695</td>\n      <td>0.014000</td>\n      <td>0.012000</td>\n    </tr>\n    <tr>\n      <th>H264-like</th>\n      <td>0.636316</td>\n      <td>0.661196</td>\n      <td>0.693363</td>\n      <td>0.702983</td>\n      <td>0.022000</td>\n      <td>0.023890</td>\n    </tr>\n    <tr>\n      <th>JPEG(50)</th>\n      <td>0.624669</td>\n      <td>0.640551</td>\n      <td>0.671669</td>\n      <td>0.681253</td>\n      <td>0.023000</td>\n      <td>0.023000</td>\n    </tr>\n    <tr>\n      <th>PGD</th>\n      <td>0.467443</td>\n      <td>0.476443</td>\n      <td>0.459703</td>\n      <td>0.483069</td>\n      <td>0.001000</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <th>U-Net</th>\n      <td>0.618846</td>\n      <td>0.606670</td>\n      <td>0.675271</td>\n      <td>0.674021</td>\n      <td>0.015000</td>\n      <td>0.015000</td>\n    </tr>\n    <tr>\n      <th>U-Net + H264-like</th>\n      <td>0.643727</td>\n      <td>0.654314</td>\n      <td>0.690201</td>\n      <td>0.698939</td>\n      <td>0.025000</td>\n      <td>0.030000</td>\n    </tr>\n    <tr>\n      <th>U-Net + JPEG(50)</th>\n      <td>0.625199</td>\n      <td>0.646903</td>\n      <td>0.674873</td>\n      <td>0.685051</td>\n      <td>0.020000</td>\n      <td>0.029000</td>\n    </tr>\n    <tr>\n      <th rowspan=\"7\" valign=\"top\">FF++ c23</th>\n      <th>Clean</th>\n      <td>0.999435</td>\n      <td>0.998306</td>\n      <td>0.999997</td>\n      <td>0.999999</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>H264-like</th>\n      <td>0.912479</td>\n      <td>0.911914</td>\n      <td>0.965480</td>\n      <td>0.962390</td>\n      <td>0.640726</td>\n      <td>0.670558</td>\n    </tr>\n    <tr>\n      <th>JPEG(50)</th>\n      <td>0.850932</td>\n      <td>0.835686</td>\n      <td>0.922041</td>\n      <td>0.916370</td>\n      <td>0.343709</td>\n      <td>0.386511</td>\n    </tr>\n    <tr>\n      <th>PGD</th>\n      <td>0.511011</td>\n      <td>0.492942</td>\n      <td>0.465106</td>\n      <td>0.449477</td>\n      <td>0.002594</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>U-Net</th>\n      <td>0.998871</td>\n      <td>0.998871</td>\n      <td>0.999997</td>\n      <td>0.999997</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>U-Net + H264-like</th>\n      <td>0.905703</td>\n      <td>0.902880</td>\n      <td>0.965967</td>\n      <td>0.962840</td>\n      <td>0.626459</td>\n      <td>0.651102</td>\n    </tr>\n    <tr>\n      <th>U-Net + JPEG(50)</th>\n      <td>0.859401</td>\n      <td>0.844156</td>\n      <td>0.923581</td>\n      <td>0.917866</td>\n      <td>0.339818</td>\n      <td>0.378729</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"import os, shutil, zipfile, json, glob, sys\nfrom pathlib import Path\n\nBASE_DIR = \"/kaggle/working\"\nART_DIR  = f\"{BASE_DIR}/phase4_export\"\nPath(ART_DIR).mkdir(parents=True, exist_ok=True)\n\nto_copy = [\n    (\"/kaggle/working/baseline_model.pth\", f\"{ART_DIR}/baseline_model.pth\"),\n    (\"/kaggle/working/robust_model.pth\",   f\"{ART_DIR}/robust_model.pth\"),\n    (\"/kaggle/working/phase4_1_eval_results.csv\", f\"{ART_DIR}/phase4_1_eval_results.csv\"),\n    (\"/kaggle/working/phase4_1_eval_results.json\", f\"{ART_DIR}/phase4_1_eval_results.json\"),\n]\n\nATTACKER_PTH = \"/kaggle/working/attacker_unet.pth\"\nif os.path.exists(ATTACKER_PTH):\n    to_copy.append((ATTACKER_PTH, f\"{ART_DIR}/attacker_unet.pth\"))\n\nfor src, dst in to_copy:\n    if os.path.exists(src):\n        os.makedirs(os.path.dirname(dst), exist_ok=True)\n        shutil.copyfile(src, dst)\n        print(\"✓\", os.path.basename(src))\n    else:\n        print(\"⚠ missing:\", src)\n\nDATA_ROOT = \"/kaggle/working/data\"\ndatasets = [\"FFpp_c23_train\", \"CelebDF_test\"]\nDATA_OUT = f\"{ART_DIR}/data\"\nfor d in datasets:\n    src = f\"{DATA_ROOT}/{d}\"\n    if os.path.isdir(src):\n        shutil.copytree(src, f\"{DATA_OUT}/{d}\", dirs_exist_ok=True)\n        print(\"✓ copied dataset:\", d)\n    else:\n        print(\"⚠ dataset folder not found:\", src)\n\ndef make_zip(zip_path, base_folder, include_data=False):\n    with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n        for p in Path(base_folder).rglob(\"*\"):\n            if p.is_dir(): \n                continue\n            rel = p.relative_to(base_folder)\n            if not include_data and str(rel).startswith(\"data/\"):\n                continue\n            z.write(p, rel)\n    print(\"→ wrote\", zip_path)\n\nmake_zip(f\"{BASE_DIR}/phase4_artifacts_light.zip\", ART_DIR, include_data=False)\nmake_zip(f\"{BASE_DIR}/phase4_artifacts_full.zip\",  ART_DIR, include_data=True)\n\nprint(\"\\nDownload from file browser:\")\nprint(\"- /kaggle/working/phase4_artifacts_light.zip   (models + 4.1 results)\")\nprint(\"- /kaggle/working/phase4_artifacts_full.zip    (+ cropped images)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:08:03.049341Z","iopub.execute_input":"2025-10-17T19:08:03.050100Z","iopub.status.idle":"2025-10-17T19:09:23.658126Z","shell.execute_reply.started":"2025-10-17T19:08:03.050073Z","shell.execute_reply":"2025-10-17T19:09:23.657290Z"}},"outputs":[{"name":"stdout","text":"✓ baseline_model.pth\n✓ robust_model.pth\n✓ phase4_1_eval_results.csv\n✓ phase4_1_eval_results.json\n✓ copied dataset: FFpp_c23_train\n✓ copied dataset: CelebDF_test\n→ wrote /kaggle/working/phase4_artifacts_light.zip\n→ wrote /kaggle/working/phase4_artifacts_full.zip\n\nDownload from file browser:\n- /kaggle/working/phase4_artifacts_light.zip   (models + 4.1 results)\n- /kaggle/working/phase4_artifacts_full.zip    (+ cropped images)\n","output_type":"stream"}],"execution_count":32}]}